{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":343604,"sourceType":"datasetVersion","datasetId":145129}],"dockerImageVersionId":30042,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install TensorFlow\n","!pip install Keras\n","!pip install numPy\n","!pip install tqdm\n","!pip install Jupyterlab"],"metadata":{"id":"eL_pz-27bi4D","outputId":"438d74fb-220d-48ce-b312-2fc31da4e675","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713797283928,"user_tz":-330,"elapsed":41377,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: TensorFlow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (1.62.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from TensorFlow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->TensorFlow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->TensorFlow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->TensorFlow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->TensorFlow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->TensorFlow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->TensorFlow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->TensorFlow) (3.2.2)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: numPy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n","Collecting Jupyterlab\n","  Downloading jupyterlab-4.1.6-py3-none-any.whl (11.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-lru>=1.0.0 (from Jupyterlab)\n","  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n","Collecting httpx>=0.25.0 (from Jupyterlab)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipykernel>=6.5.0 (from Jupyterlab)\n","  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (3.1.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (5.7.2)\n","Collecting jupyter-lsp>=2.0.0 (from Jupyterlab)\n","  Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter-server<3,>=2.4.0 (from Jupyterlab)\n","  Downloading jupyter_server-2.14.0-py3-none-any.whl (383 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.3/383.3 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyterlab-server<3,>=2.19.0 (from Jupyterlab)\n","  Downloading jupyterlab_server-2.27.0-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (0.2.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (24.0)\n","Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (2.0.1)\n","Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (6.3.3)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from Jupyterlab) (5.7.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->Jupyterlab) (4.11.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->Jupyterlab) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->Jupyterlab) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.25.0->Jupyterlab)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->Jupyterlab) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->Jupyterlab) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->Jupyterlab)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting comm>=0.1.1 (from ipykernel>=6.5.0->Jupyterlab)\n","  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->Jupyterlab) (1.6.6)\n","Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->Jupyterlab) (7.34.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->Jupyterlab) (6.1.12)\n","Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->Jupyterlab) (0.1.7)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->Jupyterlab) (1.6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->Jupyterlab) (5.9.5)\n","Collecting pyzmq>=24 (from ipykernel>=6.5.0->Jupyterlab)\n","  Downloading pyzmq-26.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (919 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.0/920.0 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.3->Jupyterlab) (2.1.5)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->Jupyterlab) (4.2.0)\n","Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (23.1.0)\n","Collecting jupyter-client>=6.1.12 (from ipykernel>=6.5.0->Jupyterlab)\n","  Downloading jupyter_client-8.6.1-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->Jupyterlab)\n","  Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n","Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->Jupyterlab)\n","  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (6.5.4)\n","Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (5.10.4)\n","Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->Jupyterlab)\n","  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (0.20.0)\n","Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (0.18.1)\n","Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->Jupyterlab) (1.7.0)\n","Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.19.0->Jupyterlab) (2.14.0)\n","Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading json5-0.9.25-py3-none-any.whl (30 kB)\n","Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.19.0->Jupyterlab) (4.19.2)\n","Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.19.0->Jupyterlab) (2.31.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->Jupyterlab) (1.2.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->Jupyterlab) (21.2.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (67.7.2)\n","Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (0.2.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab) (0.18.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel>=6.5.0->Jupyterlab) (2.8.2)\n","Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->Jupyterlab)\n","  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n","Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->Jupyterlab) (6.0.1)\n","Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->Jupyterlab)\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->Jupyterlab)\n","  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (4.9.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (4.12.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (6.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (0.4)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (0.3.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (0.10.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (1.2.1)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->Jupyterlab) (2.19.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->Jupyterlab) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->Jupyterlab) (2.0.7)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->Jupyterlab) (0.7.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (0.8.4)\n","Collecting fqdn (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Collecting isoduration (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Collecting jsonpointer>1.13 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Collecting uri-template (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n","Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab) (1.13)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->Jupyterlab) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=6.5.0->Jupyterlab) (1.16.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->Jupyterlab) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->Jupyterlab) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->Jupyterlab) (2.22)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->Jupyterlab)\n","  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n","Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, pyzmq, python-json-logger, overrides, jsonpointer, json5, jedi, h11, fqdn, comm, async-lru, jupyter-server-terminals, jupyter-client, httpcore, arrow, isoduration, ipykernel, httpx, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, Jupyterlab\n","  Attempting uninstall: pyzmq\n","    Found existing installation: pyzmq 23.2.1\n","    Uninstalling pyzmq-23.2.1:\n","      Successfully uninstalled pyzmq-23.2.1\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter-client 6.1.12\n","    Uninstalling jupyter-client-6.1.12:\n","      Successfully uninstalled jupyter-client-6.1.12\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 5.5.6\n","    Uninstalling ipykernel-5.5.6:\n","      Successfully uninstalled ipykernel-5.5.6\n","  Attempting uninstall: jupyter-server\n","    Found existing installation: jupyter-server 1.24.0\n","    Uninstalling jupyter-server-1.24.0:\n","      Successfully uninstalled jupyter-server-1.24.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.4 which is incompatible.\n","notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.1 which is incompatible.\n","notebook 6.5.5 requires pyzmq<25,>=17, but you have pyzmq 26.0.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Jupyterlab-4.1.6 arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 fqdn-1.5.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ipykernel-6.29.4 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jsonpointer-2.4 jupyter-client-8.6.1 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.0 jupyter-server-terminals-0.5.3 jupyterlab-server-2.27.0 overrides-7.7.0 python-json-logger-2.0.7 pyzmq-26.0.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from PIL import Image\n","import os\n","from pickle import dump, load\n","import numpy as np\n","import string\n","from keras.applications.xception import Xception, preprocess_input\n","from keras.preprocessing.image import load_img, img_to_array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","# from keras.layers.merge import add\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n","\n","# Importing tqdm for progress visualization\n","from tqdm import tqdm\n"],"metadata":{"trusted":true,"id":"QyicnxAoaVHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading a text file into memory\n","def load_doc(filename):\n","    # Opening the file as read only\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text"],"metadata":{"trusted":true,"id":"z2WkdIZ5aVHq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get all imgs with their captions\n","def all_img_captions(filename):\n","    file = load_doc(filename)\n","    captions = file.split('\\n')\n","    descriptions ={}\n","    for caption in captions[:-1]:\n","        img, caption = caption.split('\\t')\n","        if img[:-2] not in descriptions:\n","            descriptions[img[:-2]] = [caption]\n","        else:\n","            descriptions[img[:-2]].append(caption)\n","    return descriptions"],"metadata":{"trusted":true,"id":"RYc-qidHaVHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##Data cleaning- lower casing, removing puntuations and words containing numbers\n","def cleaning_text(captions):\n","    table = str.maketrans('','',string.punctuation)\n","    for img,caps in captions.items():\n","        for i,img_caption in enumerate(caps):\n","\n","            img_caption.replace(\"-\",\" \")\n","            desc = img_caption.split()\n","\n","            #converts to lower case\n","            desc = [word.lower() for word in desc]\n","            #remove punctuation from each token\n","            desc = [word.translate(table) for word in desc]\n","            #remove hanging 's and a\n","            desc = [word for word in desc if(len(word)>1)]\n","            #remove tokens with numbers in them\n","            desc = [word for word in desc if(word.isalpha())]\n","            #convert back to string\n","\n","            img_caption = ' '.join(desc)\n","            captions[img][i]= img_caption\n","    return captions"],"metadata":{"trusted":true,"id":"upyF59iSaVHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_vocabulary(descriptions):\n","    # build vocabulary of all unique words\n","    vocab = set()\n","\n","    for key in descriptions.keys():\n","        [vocab.update(d.split()) for d in descriptions[key]]\n","\n","    return vocab"],"metadata":{"trusted":true,"id":"czdJsBpCaVHt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#All descriptions in one file\n","def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + '\\t' + desc )\n","    data = \"\\n\".join(lines)\n","    file = open(filename,\"w\")\n","    file.write(data)\n","    file.close()"],"metadata":{"trusted":true,"id":"0g6JwmB_aVHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_text = '/content/drive/MyDrive/Capstone/Flickr8k_text'\n","dataset_images = '/content/drive/MyDrive/Capstone/Flickr8k_Dataset/Flicker8k_Dataset'"],"metadata":{"trusted":true,"id":"cE9RIx2xaVHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzbHikc_KQ6s","executionInfo":{"status":"ok","timestamp":1713796067157,"user_tz":-330,"elapsed":36380,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}},"outputId":"4aa1e247-8067-4fed-d026-4be103e84f72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#we prepare our text data\n","filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n","#loading the file that contains all data\n","#mapping them into descriptions dictionary img to 5 captions\n","descriptions = all_img_captions(filename)\n","print(\"Length of descriptions =\" ,len(descriptions))\n","\n","#cleaning the descriptions\n","clean_descriptions = cleaning_text(descriptions)\n","\n","#building vocabulary\n","vocabulary = text_vocabulary(clean_descriptions)\n","print(\"Length of vocabulary = \", len(vocabulary))\n","\n","#saving each description to file\n","save_descriptions(clean_descriptions, \"descriptions.txt\")"],"metadata":{"scrolled":true,"trusted":true,"id":"Bmw1oi5CaVHx","outputId":"111acc13-260c-4e7e-e7c6-a75537c30d71","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713797309686,"user_tz":-330,"elapsed":747,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of descriptions = 8092\n","Length of vocabulary =  8763\n"]}]},{"cell_type":"markdown","source":["Extracting feature vector"],"metadata":{"id":"itcIRI1pXTc-"}},{"cell_type":"code","source":["\n","def extract_features(directory):\n","        model = Xception( include_top=False, pooling='avg' )\n","        features = {}\n","        for img in tqdm(os.listdir(directory)):\n","            filename = directory + \"/\" + img\n","            image = Image.open(filename)\n","            image = image.resize((299,299))\n","            image = np.expand_dims(image, axis=0)\n","            #image = preprocess_input(image)\n","            image = image/127.5\n","            image = image - 1.0\n","\n","            feature = model.predict(image)\n","            features[img] = feature\n","        return features"],"metadata":{"trusted":true,"id":"DiaX2kh5aVHz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2048 feature vector\n","features = extract_features(dataset_images)\n","dump(features, open(\"features.p\",\"wb\"))"],"metadata":{"id":"i7ODChslO_Y4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = load(open('/content/drive/MyDrive/Capstone/features.p',\"rb\"))"],"metadata":{"scrolled":true,"trusted":true,"id":"YwK2hWd5aVH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load the data\n","def load_photos(filename):\n","    file = load_doc(filename)\n","    photos = file.split(\"\\n\")[:-1]\n","    return photos\n","\n","\n","def load_clean_descriptions(filename, photos):\n","    #loading clean_descriptions\n","    file = load_doc(filename)\n","    descriptions = {}\n","    for line in file.split(\"\\n\"):\n","\n","        words = line.split()\n","        if len(words)<1 :\n","            continue\n","\n","        image, image_caption = words[0], words[1:]\n","\n","        if image in photos:\n","            if image not in descriptions:\n","                descriptions[image] = []\n","            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n","            descriptions[image].append(desc)\n","\n","    return descriptions\n","\n","\n","def load_features(photos):\n","    #loading all features\n","    all_features = load(open('/content/drive/MyDrive/Capstone/features.p',\"rb\"))\n","    #selecting only needed features\n","    features = {k:all_features[k] for k in photos}\n","    return features\n"],"metadata":{"trusted":true,"id":"B1Z4EW9raVH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n","\n","#train = loading_data(filename)\n","train_imgs = load_photos(filename)\n","train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n","train_features = load_features(train_imgs)"],"metadata":{"trusted":true,"id":"U4R2z5kFaVH2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#converting dictionary to clean list of descriptions\n","def dict_to_list(descriptions):\n","    all_desc = []\n","    for key in descriptions.keys():\n","        [all_desc.append(d) for d in descriptions[key]]\n","    return all_desc\n","\n","#creating tokenizer class\n","#this will vectorise text corpus\n","#each integer will represent token in dictionary\n","\n","from keras.preprocessing.text import Tokenizer\n","\n","def create_tokenizer(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(desc_list)\n","    return tokenizer\n"],"metadata":{"trusted":true,"id":"wQr0dJpEaVH3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# give each word a index, and store that into tokenizer.p pickle file\n","tokenizer = create_tokenizer(train_descriptions)\n","dump(tokenizer, open('/content/drive/MyDrive/Capstone/tokenizer.p', 'wb'))\n","vocab_size = len(tokenizer.word_index) + 1\n","vocab_size"],"metadata":{"trusted":true,"id":"LEObWrqcaVH4","outputId":"16ca5ef0-0082-418d-9142-cbc781dbb435","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713797448737,"user_tz":-330,"elapsed":525,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7577"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["#calculate maximum length of descriptions\n","def max_length(descriptions):\n","    desc_list = dict_to_list(descriptions)\n","    return max(len(d.split()) for d in desc_list)\n","\n","max_length = max_length(descriptions)\n","max_length"],"metadata":{"trusted":true,"id":"3iU8QLGFaVH4","outputId":"a723ced0-f7e1-451e-90d2-3f5f0092dec0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713797451471,"user_tz":-330,"elapsed":4,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["features['1000268201_693b08cb0e.jpg'][0]"],"metadata":{"trusted":true,"id":"a9mSm7ZeaVH5","outputId":"ff494795-d427-4978-cd44-ea1d9813fe8d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713797454790,"user_tz":-330,"elapsed":5,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.4734095 , 0.01730889, 0.07334236, ..., 0.08557957, 0.02102294,\n","       0.2376553 ], dtype=float32)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# Define the model\n","\n","#1 Photo feature extractor - we extracted features from pretrained model Xception.\n","#2 Sequence processor - word embedding layer that handles text, followed by LSTM\n","#3 Decoder - Both 1 and 2 model produce fixed length vector. They are merged together and processed by dense layer to make final prediction"],"metadata":{"trusted":true,"id":"muoMkCotaVH5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating Data Geneator"],"metadata":{"id":"Qpff1Ahnb9fr"}},{"cell_type":"code","source":["#create input-output sequence pairs from the image description.\n","\n","#data generator, used by model.fit_generator()\n","def data_generator(descriptions, features, tokenizer, max_length):\n","    while 1:\n","        for key, description_list in descriptions.items():\n","            #retrieve photo features\n","            feature = features[key][0]\n","            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n","            yield [input_image, input_sequence], output_word\n","\n","def create_sequences(tokenizer, max_length, desc_list, feature):\n","    X1, X2, y = list(), list(), list()\n","    # walk through each description for the image\n","    for desc in desc_list:\n","        # encode the sequence\n","        seq = tokenizer.texts_to_sequences([desc])[0]\n","        # split one sequence into multiple X,y pairs\n","        for i in range(1, len(seq)):\n","            # split into input and output pair\n","            in_seq, out_seq = seq[:i], seq[i]\n","            # pad input sequence\n","            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","            # encode output sequence\n","            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","            # store\n","            X1.append(feature)\n","            X2.append(in_seq)\n","            y.append(out_seq)\n","    return np.array(X1), np.array(X2), np.array(y)"],"metadata":{"trusted":true,"id":"oJNxOULVaVH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n","a.shape, b.shape, c.shape"],"metadata":{"trusted":true,"id":"6bzTWDoAaVH7","outputId":"a493cbaa-7a96-4b2d-da32-f2c49ae027d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713797462264,"user_tz":-330,"elapsed":7,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((47, 2048), (47, 32), (47, 7577))"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from keras.utils import plot_model\n","\n","# define the captioning model\n","def define_model(vocab_size, max_length):\n","\n","    # features from the CNN model squeezed from 2048 to 256 nodes\n","    inputs1 = Input(shape=(2048,))\n","    fe1 = Dropout(0.5)(inputs1) #prevent overfitting\n","    fe2 = Dense(256, activation='relu')(fe1)\n","\n","    # LSTM sequence model\n","    inputs2 = Input(shape=(max_length,))\n","    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","    se2 = Dropout(0.5)(se1)\n","    se3 = LSTM(256)(se2)\n","\n","    # Merging both models\n","    decoder1 = add([fe2, se3])\n","    decoder2 = Dense(256, activation='relu')(decoder1)\n","    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","    # tie it together [image, seq] [word]\n","    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","    # summarize model\n","    print(model.summary())\n","    plot_model(model, to_file='model.png', show_shapes=True)\n","\n","    return model"],"metadata":{"trusted":true,"id":"2P4v-m7caVH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.layers import add"],"metadata":{"id":"-Vjls4jhje4z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train our model\n","print('Dataset: ', len(train_imgs))\n","print('Descriptions: train=', len(train_descriptions))\n","print('Photos: train=', len(train_features))\n","print('Vocabulary Size:', vocab_size)\n","print('Description Length: ', max_length)\n","\n","model = define_model(vocab_size, max_length)\n","epochs = 10\n","steps = len(train_descriptions)\n","for i in range(epochs):\n","    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n","    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n","    model.save(\"model_\" + str(i) + \".h5\")"],"metadata":{"trusted":true,"id":"PKUMYPuIaVH8","outputId":"e56b5e9f-314e-4cd4-87eb-757eabe2169c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713784608743,"user_tz":-330,"elapsed":10255875,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Dataset:  6000\n","Descriptions: train= 6000\n","Photos: train= 6000\n","Vocabulary Size: 7577\n","Description Length:  32\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_8 (InputLayer)        [(None, 32)]                 0         []                            \n","                                                                                                  \n"," input_7 (InputLayer)        [(None, 2048)]               0         []                            \n","                                                                                                  \n"," embedding_1 (Embedding)     (None, 32, 256)              1939712   ['input_8[0][0]']             \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, 2048)                 0         ['input_7[0][0]']             \n","                                                                                                  \n"," dropout_3 (Dropout)         (None, 32, 256)              0         ['embedding_1[0][0]']         \n","                                                                                                  \n"," dense_3 (Dense)             (None, 256)                  524544    ['dropout_2[0][0]']           \n","                                                                                                  \n"," lstm_1 (LSTM)               (None, 256)                  525312    ['dropout_3[0][0]']           \n","                                                                                                  \n"," add_49 (Add)                (None, 256)                  0         ['dense_3[0][0]',             \n","                                                                     'lstm_1[0][0]']              \n","                                                                                                  \n"," dense_4 (Dense)             (None, 256)                  65792     ['add_49[0][0]']              \n","                                                                                                  \n"," dense_5 (Dense)             (None, 7577)                 1947289   ['dense_4[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 5002649 (19.08 MB)\n","Trainable params: 5002649 (19.08 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","None\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["<ipython-input-45-2e3bdc1f5d6e>:13: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["6000/6000 [==============================] - 1856s 308ms/step - loss: 4.5137\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["6000/6000 [==============================] - 1826s 304ms/step - loss: 3.6687\n","6000/6000 [==============================] - 1844s 307ms/step - loss: 3.3817\n","6000/6000 [==============================] - 1733s 289ms/step - loss: 3.2125\n","6000/6000 [==============================] - 1682s 280ms/step - loss: 3.0953\n","6000/6000 [==============================] - 1778s 296ms/step - loss: 3.0057\n","6000/6000 [==============================] - 1780s 297ms/step - loss: 2.9338\n","6000/6000 [==============================] - 1629s 271ms/step - loss: 2.8845\n","6000/6000 [==============================] - 1763s 294ms/step - loss: 2.8356\n","6000/6000 [==============================] - 1721s 287ms/step - loss: 2.8010\n"]}]},{"cell_type":"code","source":["def load_photo_features(file_name, data_set):\n","  # load all features\n","  all_features = load(open(file_name, 'rb'))\n","  # filter features\n","  features = {k: all_features[k] for k in data_set}\n","  return features\n","# def load_set(file_name):\n","#   document = load_doc(file_name)\n","#   data_set = list()\n","#   # process line by line\n","#   for line in document.split('n'):\n","#     # skip empty lines\n","#     if len(line) < 1:\n","#       continue\n","#     # get the image identifier\n","#     identifier = line.split('.')[0]\n","#     data_set.append(identifier)\n","#   return set(data_set)"],"metadata":{"id":"MpV8C0sUqqaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","  actual, predicted = list(), list()\n","  # step over the whole set\n","  for key, desc_list in descriptions.items():\n","    # generate description\n","    yhat = generate_desc(model, tokenizer, photos[key], max_length)\n","    # store actual and predicted\n","    references = [d.split() for d in desc_list]\n","    actual.append(references)\n","    predicted.append(yhat.split())\n","  # calculate BLEU score\n","  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"metadata":{"id":"tLuWc6TQXsaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = '/content/drive/MyDrive/Capstone/Flickr8k_text/Flickr_8k.testImages.txt'\n","test = load_photos(filename)\n","print('Dataset: %d' % len(test))\n","# descriptions\n","test_descriptions = load_clean_descriptions('descriptions.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","# photo features\n","test_features = load_photo_features('/content/drive/MyDrive/Capstone/features.p', test)\n","print('Photos: test=%d' % len(test_features))\n","# load the model which has minimum loss, in this case it was model_9\n","filename = '/content/drive/MyDrive/Capstone/model/model_9.h5'\n","model = load_model(filename)\n","# evaluate model\n","evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gY-c5FKnSsk","executionInfo":{"status":"ok","timestamp":1713799873383,"user_tz":-330,"elapsed":672378,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}},"outputId":"2835af2d-7945-4864-87dd-5edf9afb79bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: 1000\n","Descriptions: test=1000\n","Photos: test=1000\n","BLEU-1: 0.360199\n","BLEU-2: 0.201227\n","BLEU-3: 0.139400\n","BLEU-4: 0.063090\n"]}]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.applications.xception import Xception\n","from keras.models import load_model\n","from pickle import load\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import argparse"],"metadata":{"trusted":true,"id":"zxsegLpaaVH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ap = argparse.ArgumentParser()\n","ap.add_argument('-i', '--image', required=True, help=\"Image Path\")\n","#args = vars(ap.parse_args())\n","args['image'] = dataset_images + train_imgs[0]\n","img_path = args['image']"],"metadata":{"trusted":true,"id":"7cq8bBbgaVH9","outputId":"3db0f8e1-204f-4ea8-d2d7-c73e49fedad3","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"error","timestamp":1713797487836,"user_tz":-330,"elapsed":9,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'args' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-d951044ea228>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Image Path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#args = vars(ap.parse_args())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_images\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"]}]},{"cell_type":"code","source":["def extract_features(filename, model):\n","    try:\n","        image = Image.open(filename)\n","        image = image.resize((299, 299))\n","        image = np.array(image)\n","        # for images that have 4 channels, we convert them into 3 channels\n","        if image.shape[2] == 4:\n","            image = image[..., :3]\n","        image = np.expand_dims(image, axis=0)\n","        image = image / 127.5\n","        image = image - 1.0\n","        feature = model.predict(image)\n","        return feature\n","    except Exception as e:\n","        print(\"ERROR:\", e)\n","        return None\n"],"metadata":{"trusted":true,"id":"V59YNDCpaVH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def word_for_id(integer, tokenizer):\n","    for word, index in tokenizer.word_index.items():\n","        if index == integer:\n","            return word\n","    return None"],"metadata":{"trusted":true,"id":"LbQXem_TaVH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_desc(model, tokenizer, photo, max_length):\n","    in_text = 'start'\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], maxlen=max_length)  #to make same length as max length\n","        pred = model.predict([photo,sequence], verbose=0)\n","        pred = np.argmax(pred)\n","        word = word_for_id(pred, tokenizer)\n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'end':\n","            break\n","    return in_text"],"metadata":{"trusted":true,"id":"9_CflFNDaVH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 32\n","tokenizer = load(open('/content/drive/MyDrive/Capstone/tokenizer.p',\"rb\"))\n","model = load_model('/content/drive/MyDrive/Capstone/model/model_9.h5')\n","xception_model = Xception(include_top=False, pooling=\"avg\")"],"metadata":{"trusted":true,"id":"-Yo0WdDfaVH_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize = (50, 100))\n","nOfImages = 20\n","for i in range(nOfImages):\n","    img_path = dataset_images+'/'+train_imgs[i]\n","    photo = extract_features(img_path, xception_model)\n","    img = Image.open(img_path)\n","    plt.subplot(nOfImages, 1, i + 1)\n","    plt.grid(False)\n","    plt.xticks([])\n","    plt.yticks([])\n","    description = generate_desc(model, tokenizer, photo, max_length)\n","    plt.xlabel(description)\n","    plt.imshow(img)\n","plt.show()"],"metadata":{"trusted":true,"id":"L2b0HtkbaVH_","outputId":"e49e8aed-d647-4230-e27f-04308a21a1b0","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wvt896MjA6JADU4Rbxuujy6DeEnvP3l5"},"executionInfo":{"status":"ok","timestamp":1713797771763,"user_tz":-330,"elapsed":82827,"user":{"displayName":"Arpit Goyal","userId":"10743557796385540802"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"xoRzxVqKaVIA"},"execution_count":null,"outputs":[]}]}